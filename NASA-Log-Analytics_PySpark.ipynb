{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA webserver Log Analytics using Apache Spark\n",
    "\n",
    "Apache Spark is an excellent and ideal framework for wrangling, analyzing and modeling on structured and unstructured data - at scale! In this notebook, we will be focusing on one of the most popular case studies in the industry - log analytics.\n",
    "\n",
    "Typically, server logs are a very common data source in enterprises and often contain a gold mine of actionable insights and information. Log data comes from many sources in an enterprise, such as the web, client and compute servers, applications, user-generated content, flat files. They can be used for monitoring servers, improving business and customer intelligence, building recommendation systems, fraud detection, and much more.\n",
    "\n",
    "Spark allows you to dump and store your logs in files on disk cheaply, while still providing rich APIs to perform data analysis at scale. This hands-on will show you how to use Apache Spark on real-world production logs from NASA and learn data wrangling and basic yet powerful techniques in exploratory data analysis.\n",
    "\n",
    "\n",
    "# FILL REST OF NOTEBOOK LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting up the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FindSpark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up Spark and fire up its session variables\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "    \n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up other dependencies\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test basic Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 25), match=\"I'm searching for a spark\"> 0 25\n",
      "<re.Match object; span=(25, 36), match=' in PySpark'> 25 36\n"
     ]
    }
   ],
   "source": [
    "m = re.finditer(r'.*?(spark).*?', \"I'm searching for a spark in PySpark\", re.I)\n",
    "for match in m:\n",
    "    print(match, match.start(), match.end())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Loading and Viewing the NASA Log Dataset\n",
    "Given that our data is stored in the following mentioned path, let's load it into a DataFrame. We'll do this in steps. First, we'll use sqlContext.read.text() or spark.read.text() to read the text file. This will produce a DataFrame with a single string column called value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NASA_access_log_Jul95.gz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "raw_data_files = glob.glob('*.gz')\n",
    "raw_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "(1891715, 1)\n"
     ]
    }
   ],
   "source": [
    "base_df = spark.read.text(raw_data_files)\n",
    "base_df.printSchema()\n",
    "\n",
    "print((base_df.count(), len(base_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(base_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can also convert DataFrame to RDD if needed**\n",
    "\n",
    "It is a simple one step process by calling the *.rdd* method on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df_rdd = base_df.rdd\n",
    "type(base_df_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check preview sample of the Data\n",
    "\n",
    "This will allow us to visualize what aspects of the data need to be parsed and wrangled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245                                 |\n",
      "|unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985                      |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085   |\n",
      "|burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0               |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179|\n",
      "|burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0                    |\n",
      "|burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/video/livevideo.gif HTTP/1.0\" 200 0        |\n",
      "|205.212.115.106 - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/countdown.html HTTP/1.0\" 200 3985             |\n",
      "|d104.aa.net - - [01/Jul/1995:00:00:13 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985                               |\n",
      "|129.94.144.152 - - [01/Jul/1995:00:00:13 -0400] \"GET / HTTP/1.0\" 200 7074                                              |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245',\n",
       " 'unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985',\n",
       " '199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085',\n",
       " 'burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0',\n",
       " '199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179',\n",
       " 'burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0',\n",
       " 'burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/video/livevideo.gif HTTP/1.0\" 200 0',\n",
       " '205.212.115.106 - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/countdown.html HTTP/1.0\" 200 3985',\n",
       " 'd104.aa.net - - [01/Jul/1995:00:00:13 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985',\n",
       " '129.94.144.152 - - [01/Jul/1995:00:00:13 -0400] \"GET / HTTP/1.0\" 200 7074',\n",
       " 'unicomp6.unicomp.net - - [01/Jul/1995:00:00:14 -0400] \"GET /shuttle/countdown/count.gif HTTP/1.0\" 200 40310',\n",
       " 'unicomp6.unicomp.net - - [01/Jul/1995:00:00:14 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 200 786',\n",
       " 'unicomp6.unicomp.net - - [01/Jul/1995:00:00:14 -0400] \"GET /images/KSC-logosmall.gif HTTP/1.0\" 200 1204',\n",
       " 'd104.aa.net - - [01/Jul/1995:00:00:15 -0400] \"GET /shuttle/countdown/count.gif HTTP/1.0\" 200 40310',\n",
       " 'd104.aa.net - - [01/Jul/1995:00:00:15 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 200 786']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract and take a look at Sample Logs\n",
    "\n",
    "sample_logs = [item['value'] for item in base_df.take(15)]\n",
    "sample_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building regular expression sets for lookup\n",
    "host_pattern = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "ts_pattern = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
    "method_uri_protocol_pattern = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "status_pattern = r'\\s(\\d{3})\\s'\n",
    "content_size_pattern = r'\\s(\\d+)$'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Let's now try and leverage all the regular expression patterns we previously built through testing and use the regexp_extract(...) method to build our dataframe with all the log attributes neatly extracted in their own separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|                host|           timestamp|method|            endpoint|protocol|status|content_size|\n",
      "+--------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|        199.72.81.55|01/Jul/1995:00:00...|   GET|    /history/apollo/|HTTP/1.0|   200|        6245|\n",
      "|unicomp6.unicomp.net|01/Jul/1995:00:00...|   GET| /shuttle/countdown/|HTTP/1.0|   200|        3985|\n",
      "|      199.120.110.21|01/Jul/1995:00:00...|   GET|/shuttle/missions...|HTTP/1.0|   200|        4085|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|   GET|/shuttle/countdow...|HTTP/1.0|   304|           0|\n",
      "|      199.120.110.21|01/Jul/1995:00:00...|   GET|/shuttle/missions...|HTTP/1.0|   200|        4179|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|   GET|/images/NASA-logo...|HTTP/1.0|   304|           0|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|   GET|/shuttle/countdow...|HTTP/1.0|   200|           0|\n",
      "|     205.212.115.106|01/Jul/1995:00:00...|   GET|/shuttle/countdow...|HTTP/1.0|   200|        3985|\n",
      "|         d104.aa.net|01/Jul/1995:00:00...|   GET| /shuttle/countdown/|HTTP/1.0|   200|        3985|\n",
      "|      129.94.144.152|01/Jul/1995:00:00...|   GET|                   /|HTTP/1.0|   200|        7074|\n",
      "+--------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "(1891715, 7)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "logs_df = base_df.select(regexp_extract('value', host_pattern, 1).alias('host'),\n",
    "                         regexp_extract('value', ts_pattern, 1).alias('timestamp'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 1).alias('method'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 2).alias('endpoint'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 3).alias('protocol'),\n",
    "                         regexp_extract('value', status_pattern, 1).cast('integer').alias('status'),\n",
    "                         regexp_extract('value', content_size_pattern, 1).cast('integer').alias('content_size'))\n",
    "logs_df.show(10, truncate=True)\n",
    "print((logs_df.count(), len(logs_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Missing Values\n",
    "\n",
    "Missing and null values are the bane of data analysis and machine learning. Let's see how well our data parsing and extraction logic worked. First, let's verify that there are no null rows in the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(base_df\n",
    "    .filter(base_df['value']\n",
    "                .isNull())\n",
    "    .count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero null rows in Logs\n",
    "\n",
    "There were ZERO null rows in our log file which means we have atleast something captured in every row. However, we could still have some null columns in our dataframe as all roles does not neccessarily have all column values.\n",
    "\n",
    "To test this, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_df.filter(logs_df['endpoint'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19727"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_rows_df = logs_df.filter(logs_df['host'].isNull()| \n",
    "                             logs_df['timestamp'].isNull() | \n",
    "                             logs_df['method'].isNull() |\n",
    "                             logs_df['endpoint'].isNull() |\n",
    "                             logs_df['status'].isNull() |\n",
    "                             logs_df['content_size'].isNull()|\n",
    "                             logs_df['protocol'].isNull())\n",
    "bad_rows_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying missing values in each column\n",
    "\n",
    "Here we check the total number of missing values in each column. We can do so in multiple methods using Spark. Most common ones are given below:\n",
    "\n",
    "1. UDF and agg() method\n",
    "2. Mixed use of SQL operators and isNull() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------+--------+--------+------+------------+\n",
      "|host|timestamp|method|endpoint|protocol|status|content_size|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "|   0|        0|     0|       0|       0|     1|       19727|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "|host|timestamp|method|endpoint|protocol|status|content_size|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "|   0|        0|     0|       0|       0|     1|       19727|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.functions import count, isnan, lit, when\n",
    "\n",
    "# Method 1: Use a UDF to build expressions for each column and then use agg() method to execute it.\n",
    "def count_null(col_name):\n",
    "    return spark_sum(col(col_name).isNull().cast('integer')).alias(col_name)\n",
    "\n",
    "# Build up a list of column expressions, one per column.\n",
    "exprs = [count_null(col_name) for col_name in logs_df.columns]\n",
    "\n",
    "# Run the aggregation. The *exprs converts the list of expressions into variable function arguments.\n",
    "logs_df.agg(*exprs).show()\n",
    "\n",
    "# Method 2: Use the isnan() and isNull() functions paired with other sql functions of spark\n",
    "logs_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in logs_df.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we uncovered here, it appears that only 1 value is missing in 'status' column while a huge number is missing from the 'content_size' column. In next step, we will observe the issue further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling NULLs in HTTP Status column\n",
    "\n",
    "In our parsing of logs, we analyzed them and developed a regular expression to identify the STATUS values from logs. And the expression was:\n",
    "\n",
    "**regexp_extract('value', r'\\s(\\d{3})\\s', 1).cast('integer').alias('status')**\n",
    "\n",
    "There are two possiblities for the error:\n",
    "1. There is more to status code pattern that we missed.\n",
    "2. The data point is bad.\n",
    "\n",
    "So in order to find which values are not matching this regular expression, we can use the inverse method, i.e., extract where column value does-not match the defined regular expression.\n",
    "> **Logic:** Filter the dataframe where the defined pattern of STATUS Regular Expression DOES-NOT match. In the expression below, ~ means \"not\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use rlike() on dataframe to find\n",
    "null_status_df = base_df.filter(~base_df['value'].rlike(r'\\s(\\d{3})\\s'))\n",
    "null_status_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   value|\n",
      "+--------+\n",
      "|alyssa.p|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_status_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|199.72.81.55 - - ...|\n",
      "|unicomp6.unicomp....|\n",
      "|199.120.110.21 - ...|\n",
      "|burger.letters.co...|\n",
      "|199.120.110.21 - ...|\n",
      "|burger.letters.co...|\n",
      "|burger.letters.co...|\n",
      "|205.212.115.106 -...|\n",
      "|d104.aa.net - - [...|\n",
      "|129.94.144.152 - ...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(rand(4154673701051223795) * 3)'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# F.array(F.lit(\"Retail\"), F.lit(\"SME\"),)\n",
    "\n",
    "F.array(\n",
    "    F.lit(\"Retail\"),\n",
    "    F.lit(\"SME\"),\n",
    "    F.lit(\"Cor\"),\n",
    "  ).getItem(\n",
    "    (F.rand()*3).cast(\"int\")\n",
    "  )\n",
    "\n",
    "F.rand()*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-3348f866b88c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m spark.df.withColumn(\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[1;34m\"business_vertical\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   F.array(\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'df'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.df.withColumn(\n",
    "  \"business_vertical\",\n",
    "  F.array(\n",
    "    F.lit(\"Retail\"),\n",
    "    F.lit(\"SME\"),\n",
    "    F.lit(\"Cor\"),\n",
    "  ).getItem(\n",
    "    (F.rand()*3).cast(\"int\")\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "+---+---+\n",
      "\n",
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "|   |\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Xer = spark.createDataFrame([[1,2], [3,4]], ['a', 'b'])\n",
    "Xer.show()\n",
    "\n",
    "boo = spark.createDataFrame([['']], ['a'])\n",
    "boo.show()\n",
    "\n",
    "myValues = [('BAKEL','BAKEL','1 341 2323 01415'),('BAKEL','BAKEL','2 272 7729 00307'),\n",
    "            ('BAKEL','BAKEL','2 341 1224 00549'),('BAKEL','BAKEL','2 341 1200 01194'),\n",
    "            ('BAKEL','BAKEL','1 845 0112 101159'),]\n",
    "df = sqlContext.createDataFrame(myValues,['A','B','Num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BAKEL', 'BAKEL', '1 341 2323 01415'),\n",
       " ('BAKEL', 'BAKEL', '2 272 7729 00307'),\n",
       " ('BAKEL', 'BAKEL', '2 341 1224 00549'),\n",
       " ('BAKEL', 'BAKEL', '2 341 1200 01194'),\n",
       " ('BAKEL', 'BAKEL', '1 845 0112 101159')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display((myValues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------------+-----------------+\n",
      "|    A|    B|              Num|business_vertical|\n",
      "+-----+-----+-----------------+-----------------+\n",
      "|BAKEL|BAKEL| 1 341 2323 01415|              SME|\n",
      "|BAKEL|BAKEL| 2 272 7729 00307|              SME|\n",
      "|BAKEL|BAKEL| 2 341 1224 00549|              SME|\n",
      "|BAKEL|BAKEL| 2 341 1200 01194|              Cor|\n",
      "|BAKEL|BAKEL|1 845 0112 101159|           Retail|\n",
      "+-----+-----+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "(df.withColumn(\n",
    "  \"business_vertical\",\n",
    "  F.array(\n",
    "    F.lit(\"Retail\"),\n",
    "    F.lit(\"SME\"),\n",
    "    F.lit(\"Cor\"),\n",
    "  ).getItem(\n",
    "    (F.rand()*3).cast(\"int\")\n",
    "  )\n",
    ")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------------+\n",
      "|    A|    B|              Num|\n",
      "+-----+-----+-----------------+\n",
      "|BAKEL|BAKEL| 1 341 2323 01415|\n",
      "|BAKEL|BAKEL| 2 272 7729 00307|\n",
      "|BAKEL|BAKEL| 2 341 1224 00549|\n",
      "|BAKEL|BAKEL| 2 341 1200 01194|\n",
      "|BAKEL|BAKEL|1 845 0112 101159|\n",
      "+-----+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|    A|               value|\n",
      "+-----+--------------------+\n",
      "|BAKEL|199.72.81.55 - - ...|\n",
      "|BAKEL|unicomp6.unicomp....|\n",
      "|BAKEL|199.120.110.21 - ...|\n",
      "|BAKEL|burger.letters.co...|\n",
      "|BAKEL|199.120.110.21 - ...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "sample_logs_less = [item['value'] for item in base_df.take(df.count())]\n",
    "\n",
    "sample_logs_df = sqlContext.createDataFrame(sample_logs_less, StringType())\n",
    "\n",
    "# UDF for adding a dataframe or a dataframe-column to existing dataframe\n",
    "def zipDataFrame(l, r):\n",
    "    return l.rdd.zip(r.rdd).map(lambda x: (x[0][0],x[1][0])).toDF(StructType([l.schema[0],r.schema[0]]))\n",
    "\n",
    "(zipDataFrame(df, sample_logs_df).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|199.72.81.55 - - ...|\n",
      "|unicomp6.unicomp....|\n",
      "|199.120.110.21 - ...|\n",
      "|burger.letters.co...|\n",
      "|199.120.110.21 - ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_logs_df = sqlContext.createDataFrame(sample_logs_less, StringType())\n",
    "\n",
    "sample_logs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|  date_min|  date_max|\n",
      "+----------+----------+\n",
      "|2016-11-01|2016-12-01|\n",
      "|2016-11-02|2016-12-02|\n",
      "|2016-11-03|2016-12-03|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "minDf = sc.parallelize(['2016-11-01','2016-11-02','2016-11-03']).map(lambda x: (x, )).toDF(['date_min'])\n",
    "maxDf = sc.parallelize(['2016-12-01','2016-12-02','2016-12-03']).map(lambda x: (x, )).toDF(['date_max'])\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "def zipDataFrame(l, r):\n",
    "    return l.rdd.zip(r.rdd).map(lambda x: (x[0][0],x[1][0])).toDF(StructType([l.schema[0],r.schema[0]]))\n",
    "\n",
    "combined = zipDataFrame(minDf, maxDf.select('date_max'))\n",
    "combined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date_max: string]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxDf.select('date_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[val: string]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myFloatDF.select('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|        _1|        _2|\n",
      "+----------+----------+\n",
      "|2016-11-01|2016-12-01|\n",
      "|2016-11-02|2016-12-02|\n",
      "|2016-11-03|2016-12-03|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(minDf.rdd.zip(maxDf.rdd)).map(lambda x: (x[0][0] , x[1][0])).toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|  date_min|  date_max|\n",
      "+----------+----------+\n",
      "|2016-11-01|2016-12-01|\n",
      "|2016-11-02|2016-12-02|\n",
      "|2016-11-03|2016-12-03|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(zip_df(minDf, maxDf)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Other Practice Exercises\n",
    "\n",
    "## Exploring ways to create DataFrame\n",
    "\n",
    "We can initialize a new DataFrame using flat values\n",
    "1. parallelize() method\n",
    "2. createDataFrame() method\n",
    "\n",
    "### Create DataFrame from existing column\n",
    "\n",
    "There are three good methods of doing so:\n",
    "\n",
    "1. createDataFrame() method on LIST type. (pass the TYPE in argument to avoid schema analysis error)\n",
    "2. convert list to Pandas Dataframe and then use PySpark createDataFrame() [**this avoids the need to define schema**]\n",
    "3. create RDD from list and then MAP() to column and then to DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245                                 |\n",
      "|unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985                      |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085   |\n",
      "|burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0               |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179|\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245                                 |\n",
      "|unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985                      |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085   |\n",
      "|burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0               |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179|\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245                                 |\n",
      "|unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985                      |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085   |\n",
      "|burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0               |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179|\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "# take a subset of base_df, total of 5 rows/tupples\n",
    "sample_logs_less = [item['value'] for item in base_df.take(5)]\n",
    "# display (type(sample_logs_less)) # LIST type variable\n",
    "\n",
    "# METHOD 1 : createDataFrame() method\n",
    "df_values_m1 = sqlContext.createDataFrame(sample_logs_less, StringType())\n",
    "\n",
    "df_values_m1.show(truncate=False)\n",
    "df_values_m1.printSchema()\n",
    "\n",
    "\n",
    "# METHOD 2 : using Pandas Dataframe to createDataFrame() in PySpark\n",
    "new_pd_df = pd.DataFrame(sample_logs_less) # Create Pandas Dataframe from list\n",
    "df_values_m2 = sqlContext.createDataFrame(new_pd_df, [\"value\"])\n",
    "\n",
    "df_values_m2.show(truncate=False)\n",
    "df_values_m2.printSchema()\n",
    "\n",
    "\n",
    "# METHOD 3 : create RDD out of list, then MAP() to column and use toDF() method to return DataFrame\n",
    "df_values_Rdd = sc.parallelize(sample_logs_less)\n",
    "row = Row(\"value\") # define column name\n",
    "df_values_m3 = df_values_Rdd.map(row).toDF()\n",
    "\n",
    "df_values_m3.show(truncate=False)\n",
    "df_values_m3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
